<!DOCTYPE html> <html lang="pt-br"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> Simple 2-Layer Neural Network &bull; Herb Home </title> <meta name="description" content=""> <link rel="stylesheet" href="/css/main.css"> <link rel="canonical" href="https://herbchn.github.io/SimpleNN/"> <link rel="alternate" type="application/rss+xml" title="Herb Home" href="https://herbchn.github.io/feed.xml" /> <script type="text/javascript"> var _gaq = _gaq || []; _gaq.push(['_setAccount', '']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$','$'], ['\(', '\)'] ], displayMath: [ ['$$','$$'] ], processEscapes: true, } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script> </head> <body class="single"> <main class="main"> <header class="header"> <div class="overlay"> <div class="container"> <h1 class="title"> <a href="/">Herb Home</a> </h1> <nav class="navbar"> <a href="#" class="menu-icon"> <span></span> <span></span> <span></span> </a> <ul class="nav"> <li><a href="/about/">About</a></li> <li><a href="https://github.com/Herbchn" target="_blank">Github</a></li> </ul> </nav> </div> </div> </header> <article class="post container card"> <header class="post-header"> <h1 class="post-title">Simple 2-Layer Neural Network</h1> <span class="post-meta"> <time class="post-date" datetime="2016-09-20">Sep 20, 2016</time> <span class="post-author">by Herb</span> </span> </header> <div class="post-content"> <p><img src="https://herbchn.github.io/images/2-layer-nn.jpg" alt="2-layer-nn" /></p> <h2 id="section">网络结构</h2> <ul> <li> <p>Input Layer <br /> $X :\ $ 样本特征 $\ shape=(n,\ dim)$ <br /> $n :\ $ 样本个数 <br /> $dim :$ 样本的维度（即：输入层节点个数）</p> </li> <li> <p>Hidden Layer 1 <br /> $W_{1} :\ $ 权重矩阵 $\ shape=(dim,\ num_{h1})$ <br /> $\quad\quad\ \ $ $num_{h1} :\ $隐藏层1的隐藏节点个数 <br /> $b_{1} :\ $ 偏置向量 $ shape=(1,\ num_{h1})$ <br /> $z_{1} :\ $ 加权后得到的神经元输入 $ shape=(n,\ num_{h1})$ <br /> $a_{1} :\ $ 神经元输出的激活函数值 $ shape=(n,\ num_{h1})$</p> </li> <li> <p>Hidden Layer 2 <br /> $W_{2} :\ $ 权重矩阵 $\ shape=(num_{h1},\ num_{h2})$ <br /> $\quad\quad\ \ $ $num_{h2}\ :\ $隐藏层2的隐藏节点个数 <br /> $b_{2} :\ $ 偏置向量 $\ shape=(1,\ num_{h2})$ <br /> $z_{2} :\ $ 加权后得到的神经元输入 $\ shape=(n,\ num_{h2})$ <br /> $a_{2} :\ $ 神经元输出的激活函数值 $\ shape=(1,\ num_{h2})$</p> </li> <li> <p>Output Layer <br /> $W_{3} :\ $ 权重矩阵 $ shape=(num_{h2},\ num_{h3})$ <br /> $\quad\quad\ \ $ $num_{h3}\ :\ $输出层节点个数（$num_{h3}$个分类） <br /> $b_{3} :\ $ 偏置向量 $ shape=(1,\ num_{h3})$ <br /> $z_{3} :\ $ 加权后得到的神经元输入 $ shape=(n,\ num_{h3})$ <br /> $a_{3} :\ $ 经Softmax函数计算得到的输出值 $ shape=(n,\ num_{h3})$</p> </li> </ul> <h2 id="forward-propagation">Forward Propagation</h2> <ul> <li>计算第一层输入值： $z_{1}=X \cdot W_{1} + b_{1}$</li> <li>计算第一层激活值： $a_{1}=tanh(z_{1})$</li> <li>计算第二层输入值： $z_{2}=a_{1} \cdot W_{2} + b_{2}$</li> <li>计算第二层激活值：$a_{2}=tanh(z_{2})$</li> <li>计算输出层输入值：$z_{3}=a_{2} \cdot W_{3} + b_{3}$</li> <li>计算输出层激活值：$a_{3}=softmax(z_{3}) = \hat y$</li> </ul> <h2 id="back-propagation">Back Propagation</h2> <ul> <li> <p>优化目标函数：交叉熵</p> <p>$J(y,\ \hat y)= - \frac {1}{N} \sum_{n \in N} \sum_{i \in C} y_{(n,\ i)} log \hat y_{(n,\ i)}$</p> </li> <li> <p>令：$\delta_{3} = \hat y - y $ 表示当前模型的预测误差（向量）</p> </li> <li> <p>则：$\frac {\partial J}{\partial W_{3}} = a_{2}^{\mathrm {T}} \cdot \delta_{3}\quad $ 且有： <br /> $\frac {\partial J}{\partial b_{3}} = \delta_{3}$</p> </li> <li> <p>令：$\delta_{2} = \delta_{3} \cdot W_{3}^{\mathrm{T}} *[1-tanh^2(z_{2})]$ 表示残差</p> </li> <li> <p>则：$\frac {\partial J}{\partial W_{2}} = a_{1}^{\mathrm {T}} \cdot \delta_{2} \quad $ 且有： <br /> $\frac {\partial J}{\partial b_{2}} = \delta_{2}$</p> </li> <li> <p>令：$\delta_{1} = \delta_{2} \cdot W_{2}^{\mathrm{T}} * [1-tanh^2(z_{1})]$ 表示残差</p> </li> <li> <p>则：$\frac {\partial J}{\partial W_{1}} = X^{\mathrm {T}} \cdot \delta_{1}\quad $ 且有：<br /> $\frac {\partial J}{\partial b_{1}} = \delta_{1}$</p> </li> </ul> <h2 id="book--blog">Book &amp; Blog</h2> <ul> <li><a href="http://neuralnetworksanddeeplearning.com/">《Neural Networks and Deep Learning》</a>, Michael Nielsen. <br /> 中译本： <br /> <a href="https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details">《「 Neural Networks and Deep Learning 」中文翻译》</a>，HIT-SCIR. <br /> <a href="http://wiki.jikexueyuan.com/project/neural-networks-and-deep-learning-zh-cn/">《神经网络与深度学习》</a>，极客学院。 <blockquote> <p>HIT-SCIR：哈尔滨工业大学社会计算与信息检索研究中心(hit-scir)，主任刘挺教授,副主任秦兵教授。 <br /> 极客学院：其下还有《TensorFlow 官方文档中文版》。</p> </blockquote> </li> </ul> <h2 id="section-1">代码</h2><pre><code class="language-python"># This function learns parameters for the neural network and returns the model.
# - nn_hdim: Number of nodes in the hidden layer
# - num_passes: Number of passes through the training data for gradient descent
# - print_loss: If True, print the loss every 1000 iterations
def build_model(nn_hdim, num_passes=20000, print_loss=False):
    
    # Initialize the parameters to random values. We need to learn these.
    np.random.seed(0)
    W1 = np.random.randn(nn_input_dim, nn_hdim[0]) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim[0]))
    W2 = np.random.randn(nn_hdim[0], nn_hdim[1]) / np.sqrt(nn_hdim[0])
    b2 = np.zeros((1, nn_hdim[1]))
    W3 = np.random.randn(nn_hdim[1], nn_output_dim)
    b3 = np.zeros((1, nn_output_dim))

    # This is what we return at the end
    model = {}
    
    # Gradient descent. For each batch...
    for i in xrange(0, num_passes):

        # Forward propagation
        z1 = X.dot(W1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(W2) + b2
        a2 = np.tanh(z2)
        z3 = a2.dot(W3) + b3
        exp_scores = np.exp(z3)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        # Backpropagation
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        dW3 = (a2.T).dot(delta3)
        db3 = np.sum(delta3, axis=0)
        delta2 = delta3.dot(W3.T) * (1 - np.power(a2, 2))
        dW2 = (a1.T).dot(delta2)
        db2 = np.sum(delta2, axis=0)
        delta1 = delta2.dot(W2.T) * (1 - np.power(a1, 2))
        dW1 = np.dot(X.T, delta1)
        db1 = np.sum(delta1, axis=0)

        # Add regularization terms (b1 and b2 don't have regularization terms)\
        dW3 += reg_lambda * W3
        dW2 += reg_lambda * W2
        dW1 += reg_lambda * W1

        # Gradient descent parameter update
        W1 += -epsilon * dW1
        b1 += -epsilon * db1
        W2 += -epsilon * dW2
        b2 += -epsilon * db2
        W3 += -epsilon * dW3
        b3 += -epsilon * db3
        
        # Assign new parameters to the model
        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3' : W3, 'b3' : b3}
        
        # Optionally print the loss.
# This is expensive because it uses the whole dataset, so we don't want to do it too often.*
        if print_loss and i % 1000 == 0:
            print "Loss after iteration %i: %f" %(i, calculate_loss(model))
    
    return model

# start
hidden_layer_dimensions = [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [20, 20], [50, 50]]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    model = build_model(nn_hdim, print_loss = True)
</code></pre><aside class="share"> <span>Share this: </span> <a href="http://twitter.com/share?text=Simple 2-Layer Neural Network&amp;url=https://herbchn.github.io/SimpleNN/&amp;hashtags=web,dev,blog,soudev&amp;via=nandomoreirame" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;"> <i class="fa fa-twitter-square"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://herbchn.github.io/SimpleNN/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;"> <i class="fa fa-facebook-square"></i> </a> </aside> <hr> <aside id="comments" class="disqus"> <h2 class="txt-center">Comments</h2> <div id="disqus_thread"></div> <script type="text/javascript"> var disqus_shortname = ''; var disqus_identifier = '/SimpleNN'; var disqus_title = 'Simple 2-Layer Neural Network'; var disqus_url = 'https://herbchn.github.io'; /*var disqus_developer = 1;*/ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript> Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a> </noscript> </aside> </div> </article> <footer class="footer t-center"> <div class="container"> <div class="social-icons"> <ul class="text-left"> <!-- <li><a href="" target="_blank"><i class="fa fa-facebook"></i></a></li> --> <li><a href="https://twitter.com/Herb_Kitty/" target="_blank"><i class="fa fa-twitter"></i></a></li> </ul> </div> <small>&copy; 2016 All rights reserved. Made with <a href="http://jekyllrb.com" target="_blank">Jekyll</a></small> </div> </footer> </main> </body> </html></body></html>

