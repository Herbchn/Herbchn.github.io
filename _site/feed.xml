<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Herb Home</title>
    <description>Hi</description>
    <link>https://herbchn.github.io/</link>
    <atom:link href="https://herbchn.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 23 Oct 2016 22:27:36 +0800</pubDate>
    <lastBuildDate>Sun, 23 Oct 2016 22:27:36 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Logistic Regression--Theano Based</title>
        <description>&lt;h1 id=&quot;theano&quot;&gt;关于theano的一些认识&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;theano四个概念：
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;符号变量&lt;br /&gt;
  符号变量是theano中定义表达式的基本组成，其特点是不保持实际的数值，作为占位符而存在。&lt;br /&gt;
  声明： theano.scalar, theano.matrix, …&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;op&lt;br /&gt;
  	op 是定义在符号变量上的一组运算。&lt;br /&gt;
  例子： x, y 是符号变量， z = x + y 是一个op&lt;br /&gt;
  	注意： theano将符号表达式解析为graph，该图的节点包括：符号变量, op, apply.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;shared 变量&lt;br /&gt;
  	共享变量是指可以在不同的function间共享的变量，其可以保存值，需要在声明时指定需要共享的变量的具体的数值。通常用于定义模型参数，以便与保存和更新。&lt;br /&gt;
  声明：w = theano.shared(value=, name=””).  value是需要共享的变量， name是为其指定的名字（可以不指定）。&lt;br /&gt;
  取值：w.get_value()&lt;br /&gt;
  赋值：w.set_value(new_value)&lt;br /&gt;
  更新：theano.function([], target, updates=[(w, new_w)]) or function([], target, updates={w:new_w})&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;function&lt;br /&gt;
  	function是符号表达式执行的入口，通过functon为符号变量赋值，并执行相应的操作。&lt;br /&gt;
  声明：theano.function(inputs, outputs, updates, givens). inputs是一组符号变量的list，作为function的输入; outpus是需要的输出，可以是一个符号变量（or op），也可以list; updates对共享变量进行更新; givens是一个pairs(Var1, Var2)的list, tuple or dict，用第Var2来替换图中的节点Var1.&lt;br /&gt;
  注意：可以采用 In 来为符号来为function的输入指定默认值，如： theano.function([x, theano.In(y, value=1)], x+y)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;定义输入&lt;br /&gt;
  定义输入 $x$ 和输出 $y$，其中前者是矩阵，维度0代表样本数，维度1代表样本的表示；后者是向量，对应于样本 $y$ 的正确输出。
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# define the input
x = T.dmatrix(name=&quot;x&quot;)
y = T.dvector(name=&quot;y&quot;)  
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;初始化模型参数&lt;br /&gt;
  根据输入的样本的表示的维度 $dim$ ，初始化模型参数 $w$ 和 $b$，并进行shared。 前者为向量，维度为 $dim$；后者为标量。
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# 如果未给定w，則随机初始化
if w == None:
  self.w = shared(rng.randn(dim), name=&quot;w&quot;)
else:
  self.w = shared(w, name = &quot;w&quot;)
# 如果未给定b， 则随机初始化
if b == None:
  self.b = shared(0., name = &quot;b&quot;)
else:
  self.b = shared(b, name = &quot;b&quot;)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;定义LogisticRegression， Prediction， error， cost
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;LogisticRegression函数定义： $g(x) = \frac {1.0} {1.0 + e^{-x}}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Prediction函数定义： $predict = \begin{cases} 1,\ g(x) &amp;gt; 0.5 \ 0,\ g(x) &amp;lt; 0.5  \end{cases}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;error函数定义： $error = -\frac{1}{N} \sum_{i=1}^{N} y_i * \log g_i$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;cost是在cross entropy的基础上，为 $w$ 加上一个平方正则化项。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# define the regression(g), prediction, error, cost
self.g = 1./(1. + T.exp(-T.dot(x,self.w) - self.b))
self.prediction = self.g &amp;gt; 0.5
self.error = -y*T.log(self.g) - (1-y)*T.log(1-self.g)
cost = self.error.mean() + regular * (self.w ** 2).sum()
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;计算梯度
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# compute gradient
gw, gb = T.grad(cost, [self.w, self.b])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;定义入口函数
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# define reg, train, test
self.predict = theano.function([x], self.prediction)
self.train = theano.function([x, y], self.error, updates = [(self.w, self.w - step * gw), (self.b, self.b - step * gb)])
self.test = theano.function([x,y], [self.prediction, 1 - T.mean(T.neq(self.prediction, y))])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 23 Oct 2016 00:00:00 +0800</pubDate>
        <link>https://herbchn.github.io/Logression-Theano-Based/</link>
        <guid isPermaLink="true">https://herbchn.github.io/Logression-Theano-Based/</guid>
        
        
        <category>herb</category>
        
      </item>
    
      <item>
        <title>Simple 2-Layer Neural Network</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://herbchn.github.io/images/2-layer-nn.jpg&quot; alt=&quot;2-layer-nn&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;网络结构&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Input Layer  &lt;br /&gt;
$X :\ $ 样本特征   $\ shape=(n,\ dim)$  &lt;br /&gt;
$n :\ $ 样本个数  &lt;br /&gt;
$dim :$ 样本的维度（即：输入层节点个数）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hidden Layer 1  &lt;br /&gt;
$W_{1} :\ $ 权重矩阵  $\ shape=(dim,\ num_{h1})$   &lt;br /&gt;
$\quad\quad\ \ $ $num_{h1} :\ $隐藏层1的隐藏节点个数  &lt;br /&gt;
$b_{1} :\ $ 偏置向量  $ shape=(1,\ num_{h1})$  &lt;br /&gt;
$z_{1} :\ $ 加权后得到的神经元输入  $ shape=(n,\ num_{h1})$  &lt;br /&gt;
$a_{1} :\ $ 神经元输出的激活函数值  $ shape=(n,\ num_{h1})$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hidden Layer 2   &lt;br /&gt;
$W_{2} :\ $ 权重矩阵  $\  shape=(num_{h1},\ num_{h2})$  &lt;br /&gt;
$\quad\quad\ \ $ $num_{h2}\ :\ $隐藏层2的隐藏节点个数    &lt;br /&gt;
$b_{2} :\ $ 偏置向量  $\  shape=(1,\ num_{h2})$  &lt;br /&gt;
$z_{2} :\ $ 加权后得到的神经元输入  $\  shape=(n,\ num_{h2})$  &lt;br /&gt;
$a_{2} :\ $ 神经元输出的激活函数值  $\  shape=(1,\ num_{h2})$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output Layer  &lt;br /&gt;
$W_{3} :\ $ 权重矩阵  $ shape=(num_{h2},\ num_{h3})$  &lt;br /&gt;
$\quad\quad\ \ $ $num_{h3}\ :\ $输出层节点个数（$num_{h3}$个分类）  &lt;br /&gt;
$b_{3} :\ $ 偏置向量  $ shape=(1,\ num_{h3})$  &lt;br /&gt;
$z_{3} :\ $ 加权后得到的神经元输入  $ shape=(n,\ num_{h3})$  &lt;br /&gt;
$a_{3} :\ $ 经Softmax函数计算得到的输出值  $ shape=(n,\ num_{h3})$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward Propagation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;计算第一层输入值： $z_{1}=X \cdot W_{1} + b_{1}$&lt;/li&gt;
  &lt;li&gt;计算第一层激活值： $a_{1}=tanh(z_{1})$&lt;/li&gt;
  &lt;li&gt;计算第二层输入值： $z_{2}=a_{1} \cdot W_{2} + b_{2}$&lt;/li&gt;
  &lt;li&gt;计算第二层激活值：$a_{2}=tanh(z_{2})$&lt;/li&gt;
  &lt;li&gt;计算输出层输入值：$z_{3}=a_{2} \cdot W_{3} + b_{3}$&lt;/li&gt;
  &lt;li&gt;计算输出层激活值：$a_{3}=softmax(z_{3}) = \hat y$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;back-propagation&quot;&gt;Back Propagation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;优化目标函数：交叉熵&lt;/p&gt;

    &lt;p&gt;$J(y,\ \hat y)= - \frac {1}{N} \sum_{n \in N} \sum_{i \in C} y_{(n,\ i)} \log \hat y_{(n,\ i)}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;令：$\delta_{3} = \hat y - y $ 表示当前模型的预测误差（向量）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;则：$\frac {\partial J}{\partial W_{3}} = a_{2}^{\mathrm {T}} \cdot \delta_{3}\quad $  且有： &lt;br /&gt;
$\frac {\partial J}{\partial b_{3}} = \delta_{3}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;令：$\delta_{2} = \delta_{3} \cdot W_{3}^{\mathrm{T}} *[1-tanh^2(z_{2})]$  表示残差&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;则：$\frac {\partial J}{\partial W_{2}} = a_{1}^{\mathrm {T}} \cdot \delta_{2} \quad $ 且有： &lt;br /&gt;
$\frac {\partial J}{\partial b_{2}} = \delta_{2}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;令：$\delta_{1} = \delta_{2} \cdot W_{2}^{\mathrm{T}} * [1-tanh^2(z_{1})]$ 表示残差&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;则：$\frac {\partial J}{\partial W_{1}} = X^{\mathrm {T}} \cdot \delta_{1}\quad $ 且有：&lt;br /&gt;
$\frac {\partial J}{\partial b_{1}} = \delta_{1}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;book--blog&quot;&gt;Book &amp;amp; Blog&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/&quot;&gt;《Neural Networks and Deep Learning》&lt;/a&gt;, Michael Nielsen.  &lt;br /&gt;
中译本：  &lt;br /&gt;
&lt;a href=&quot;https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details&quot;&gt;《「 Neural Networks and Deep Learning 」中文翻译》&lt;/a&gt;，HIT-SCIR.   &lt;br /&gt;
&lt;a href=&quot;http://wiki.jikexueyuan.com/project/neural-networks-and-deep-learning-zh-cn/&quot;&gt;《神经网络与深度学习》&lt;/a&gt;，极客学院。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;HIT-SCIR：哈尔滨工业大学社会计算与信息检索研究中心(hit-scir)，主任刘挺教授,副主任秦兵教授。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;极客学院：其下还有《TensorFlow 官方文档中文版》。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;代码&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# This function learns parameters for the neural network and returns the model.
# - nn_hdim: Number of nodes in the hidden layer
# - num_passes: Number of passes through the training data for gradient descent
# - print_loss: If True, print the loss every 1000 iterations
def build_model(nn_hdim, num_passes=20000, print_loss=False):
    
    # Initialize the parameters to random values. We need to learn these.
    np.random.seed(0)
    W1 = np.random.randn(nn_input_dim, nn_hdim[0]) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim[0]))
    W2 = np.random.randn(nn_hdim[0], nn_hdim[1]) / np.sqrt(nn_hdim[0])
    b2 = np.zeros((1, nn_hdim[1]))
    W3 = np.random.randn(nn_hdim[1], nn_output_dim)
    b3 = np.zeros((1, nn_output_dim))

    # This is what we return at the end
    model = {}
    
    # Gradient descent. For each batch...
    for i in xrange(0, num_passes):

        # Forward propagation
        z1 = X.dot(W1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(W2) + b2
        a2 = np.tanh(z2)
        z3 = a2.dot(W3) + b3
        exp_scores = np.exp(z3)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        # Backpropagation
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        dW3 = (a2.T).dot(delta3)
        db3 = np.sum(delta3, axis=0)
        delta2 = delta3.dot(W3.T) * (1 - np.power(a2, 2))
        dW2 = (a1.T).dot(delta2)
        db2 = np.sum(delta2, axis=0)
        delta1 = delta2.dot(W2.T) * (1 - np.power(a1, 2))
        dW1 = np.dot(X.T, delta1)
        db1 = np.sum(delta1, axis=0)

        # Add regularization terms (b1 and b2 don&#39;t have regularization terms)\
        dW3 += reg_lambda * W3
        dW2 += reg_lambda * W2
        dW1 += reg_lambda * W1

        # Gradient descent parameter update
        W1 += -epsilon * dW1
        b1 += -epsilon * db1
        W2 += -epsilon * dW2
        b2 += -epsilon * db2
        W3 += -epsilon * dW3
        b3 += -epsilon * db3
        
        # Assign new parameters to the model
        model = { &#39;W1&#39;: W1, &#39;b1&#39;: b1, &#39;W2&#39;: W2, &#39;b2&#39;: b2, &#39;W3&#39; : W3, &#39;b3&#39; : b3}
        
        # Optionally print the loss.
# This is expensive because it uses the whole dataset, so we don&#39;t want to do it too often.*
        if print_loss and i % 1000 == 0:
            print &quot;Loss after iteration %i: %f&quot; %(i, calculate_loss(model))
    
    return model

# start
hidden_layer_dimensions = [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [20, 20], [50, 50]]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    model = build_model(nn_hdim, print_loss = True)
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Tue, 20 Sep 2016 00:00:00 +0800</pubDate>
        <link>https://herbchn.github.io/SimpleNN/</link>
        <guid isPermaLink="true">https://herbchn.github.io/SimpleNN/</guid>
        
        
        <category>herb</category>
        
      </item>
    
  </channel>
</rss>
